{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of California Los Angeles  \n",
    "Master of Quantitative Economics -MQE-  \n",
    "ECON-434-Machine Learning and Big Data for Economists\n",
    "\n",
    "<p style='text-align: right;'>Luis Alejandro Samayoa Alvarado </p>\n",
    "<p style='text-align: right;'>UID 506140191</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h1>Homework No.3</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Provide a python code to calculate the double Lasso estimator as well as the corresponding asymptotic standard errors.\n",
    "\n",
    "* *First, I will define a function to estimate the lambda using the BCCH method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BCCH function\n",
    "def BCCH(X, Y, c=1.1, alpha=0.05):\n",
    "    X_copy = np.array(X)\n",
    "    Y_copy = np.array(Y)\n",
    "\n",
    "    if len(X_copy) != len(Y_copy):\n",
    "        raise ValueError(\"Length of X and Y must be the same\")\n",
    "\n",
    "    n, p = X_copy.shape\n",
    "\n",
    "    Y_reshaped = np.tile(Y_copy, (p, 1)).T\n",
    "    maximum = np.max((np.mean((X_copy * Y_reshaped) ** 2, axis=1)) ** 0.5)\n",
    "    ppf = norm.ppf(1 - alpha / (2 * p))\n",
    "    lambda_pilot = (c / (n ** 0.5)) * ppf * maximum\n",
    "    lasso = Lasso(alpha=lambda_pilot)                                 \n",
    "    lasso.fit(X_copy, Y_copy)\n",
    "    Y_prediction = lasso.predict(X_copy).reshape(-1, 1)\n",
    "    e = Y_copy - Y_prediction.flatten()\n",
    "    e_reshaped = np.tile(e, (p, 1)).T\n",
    "    new_maximum = np.max((np.mean((X_copy * e_reshaped) ** 2, axis=1)) ** 0.5)\n",
    "    lambda_final = (c / (n ** 0.5)) * ppf * new_maximum\n",
    "    return lambda_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional way\n",
    "\n",
    "* *Steps to perform double lasso following the slides instructions.*\n",
    "* *Estimation of alpha and estandar desviation using formulas in slides.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.5924\n",
      "Estimated standard error: 0.1066\n",
      "Confidence interval: (0.3835, 0.8013)\n"
     ]
    }
   ],
   "source": [
    "def double_lasso(Y, D, Z):\n",
    "    # Step 1: LASSO of Y on D and Z\n",
    "    X_y = np.concatenate((D.reshape(-1, 1), Z), axis=1)\n",
    "    bcch_y = BCCH(X_y, Y)\n",
    "    lasso_y = Lasso(alpha=bcch_y)\n",
    "    lasso_y.fit(X_y, Y)\n",
    "    gammas_hat = lasso_y.coef_[1:]\n",
    "    predicted_y = lasso_y.predict(X_y)\n",
    "    e_residuals = Y - predicted_y\n",
    "\n",
    "    # Step 2: LASSO of D on Z\n",
    "    bcch_d = BCCH(Z, D)\n",
    "    lasso_d = Lasso(alpha=bcch_d)\n",
    "    lasso_d.fit(Z, D)\n",
    "    predicted_d = lasso_d.predict(Z)\n",
    "    v_residuals = D - predicted_d\n",
    "    \n",
    "    # Step 3: Estimate alpha\n",
    "    seudo_residuals = Y - np.dot(Z, gammas_hat) # Y - Z*gammas_hat\n",
    "    numerator = np.sum(seudo_residuals * v_residuals)\n",
    "    denominator = np.sum(D * v_residuals)\n",
    "    estimated_alpha = numerator / denominator\n",
    "\n",
    "    # Calculate estimated standard error\n",
    "    var_num = np.mean((e_residuals * v_residuals)** 2)\n",
    "    var_den = (np.mean(v_residuals**2))**2\n",
    "    \n",
    "    var_error = var_num / var_den\n",
    "    std_error = np.sqrt(var_error/len(Y))\n",
    "    return estimated_alpha, std_error\n",
    "\n",
    "# Example usage:\n",
    "# Generate synthetic data\n",
    "np.random.seed(150)\n",
    "n = 1000\n",
    "p = 10\n",
    "\n",
    "D = np.random.randn(n)\n",
    "Z = np.random.randn(n, p)\n",
    "Y = 0.5 * D + np.dot(Z, np.ones(p)) + np.random.randn(n)\n",
    "\n",
    "# Run double LASSO\n",
    "estimated_alpha, estimated_std_error = double_lasso(Y, D, Z)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error:\", estimated_std_error.round(4))\n",
    "min = estimated_alpha - 1.96 * estimated_std_error\n",
    "max = estimated_alpha + 1.96 * estimated_std_error\n",
    "print(\"Confidence interval:\", (min.round(4), max.round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Implementation\n",
    "\n",
    "* *Compare with the alternative implementation, using an Ordinary Least Square in the last step to obtain the standard errors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.5924\n",
      "Estimated standard error of alpha: 0.1065\n",
      "95% Confidence interval of alpha: (0.3836, 0.8012)\n"
     ]
    }
   ],
   "source": [
    "def double_lasso_OLS(Y, D, Z):\n",
    "    # Step 1: LASSO of Y on D and Z\n",
    "    X = np.concatenate((D.reshape(-1, 1), Z), axis=1)\n",
    "    lambda_y = BCCH(X, Y)\n",
    "    lasso_y = Lasso(alpha=lambda_y)\n",
    "    lasso_y.fit(X, Y)\n",
    "    selected_variables_y = X[:, lasso_y.coef_ != 0]\n",
    "\n",
    "    # Step 2: LASSO of D on Z\n",
    "    D_lasso = D.flatten()\n",
    "    lambda_d = BCCH(Z, D_lasso)\n",
    "    lasso_d = Lasso(alpha=lambda_d)\n",
    "    lasso_d.fit(Z, D_lasso)\n",
    "    selected_variables_d = Z[:, lasso_d.coef_ != 0]\n",
    "\n",
    "    # Step 3: OLS of Y on D and selected variables from steps 1 and 2\n",
    "    D_reshaped = D.reshape(-1, 1)\n",
    "    X_selected = np.concatenate((D_reshaped, selected_variables_y, selected_variables_d), axis=1)\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(X_selected, Y)\n",
    "    \n",
    "    # Get standard errors of the coefficients\n",
    "    residuals = Y - ols.predict(X_selected)\n",
    "    mse_residuals = np.mean(residuals ** 2)\n",
    "    X_selected_T = X_selected.T\n",
    "    var_cov_matrix = mse_residuals * np.linalg.inv(np.dot(X_selected_T, X_selected))\n",
    "    std_errors = np.sqrt(np.diag(var_cov_matrix))\n",
    "    \n",
    "    # Print coefficient alpha, standard error, and confidence interval\n",
    "    coefficient_alpha = ols.coef_[0]\n",
    "    standard_error_alpha = std_errors[0]\n",
    "    ci_min = coefficient_alpha - 1.96 * standard_error_alpha\n",
    "    ci_max = coefficient_alpha + 1.96 * standard_error_alpha\n",
    "    \n",
    "    return coefficient_alpha, standard_error_alpha, (ci_min, ci_max)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(150)\n",
    "n = 1000\n",
    "p = 10\n",
    "D = np.random.randn(n)\n",
    "Z = np.random.randn(n, p)\n",
    "Y = 0.5 * D + np.dot(Z, np.ones(p)) + np.random.randn(n)\n",
    "\n",
    "# Run double LASSO regression\n",
    "estimated_alpha, estimated_std_error, ci_alpha = double_lasso_OLS(Y, D, Z)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error of alpha:\", estimated_std_error.round(4))\n",
    "print(\"95% Confidence interval of alpha:\", (ci_alpha[0].round(4), ci_alpha[1].round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *As we can see, both methods to do Double Lasso returns the same estimated alpha and almost the same estandard error.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes information on both the transit agencies and on the Metropolitan Statistical Areas (MSA) where they operate. For each time period, the dataset contains values for the following variables:\n",
    "\n",
    "1. UPTTotal â€“ the number of rides for the public transit agency;\n",
    "2. treatUberX - a dummy for Uber presence in the corresponding MSA;\n",
    "3. treatGTNotStd - a variable measuring google search intensity for Uber in the corresponding MSA;\n",
    "4. popestimate - population in the corresponding MSA;\n",
    "5. employment - employment in the corresponding MSA;\n",
    "6. aveFareTotal - average fare for the public transit agency;\n",
    "7. VRHTTotal - vehicle hours for the public transit agency;\n",
    "8. VOMSTotal - number of vehicles employed by the public transit agency;\n",
    "9. VRMTotal - vehicle miles for the public transit agency;\n",
    "10. gasPrice - gas price in the corresponding MSA.\n",
    "\n",
    "In this dataset, treatUber and treatGTNotStd is qualitative and quantitative measures for the same thing: Uber presense in the MSA. We can run regressions using either of these two variables and then check whether results are robust if the other variable is used.\n",
    "\n",
    "There are two variations in this dataset that allow us to study the effect of Uber on public transit. First, in any given time period, Uber is present in some MSAs but not in others. We can thus study the effect of Uber by comparing these MSAs. Second, for any given MSA, we have data on time periods both before and after Uber was introduced in this MSA. We can thus study the effect 1of Uber by comparing these time periods. By working with panel data, we are able to employ both variations at the same time.\n",
    "\n",
    "To study the effect of Uber on public transit, we let Yit be UPTTotal, Dit be either treatUberX or treatGTNotStd, and Wit be the vector including remaining variables: popestimate, employment, aveFareTotal, VRHTTotal, VOMSTotal, VRMTotal, gasPrice. We then run the following regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from linearmodels.panel import PanelOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"uber_dataset.csv\", index_col=0)\n",
    "\n",
    "# Drop treatGTNotStd variable\n",
    "data = data.drop(columns='treatGTNotStd')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# If treatUberX is greater than 0.5, set it to 1, if not, set it to 0\n",
    "data['treatUberX'] = (data['treatUberX'] > 0.5).astype(int)\n",
    "\n",
    "# Calculate the median population across all entities\n",
    "median_population = data.groupby('dateSurvey')['popestimate'].median()\n",
    "\n",
    "# Merge the median population back to the original dataframe\n",
    "data = data.reset_index().merge(median_population.rename('median_pop'), on='dateSurvey')\n",
    "\n",
    "# Create the dummy variable P_{it}\n",
    "data['P'] = (data['popestimate'] > data['median_pop']).astype(int)\n",
    "\n",
    "# Calculate the median rides across all times\n",
    "median_rides = data.groupby('dateSurvey')['UPTTotal'].median()\n",
    "\n",
    "# Merge the median population back to the original dataframe\n",
    "data = data.reset_index().merge(median_rides.rename('median_ride'), on='dateSurvey')\n",
    "\n",
    "# Create the dummy variable F_{it}\n",
    "data['F'] = (data['UPTTotal'] > data['median_ride']).astype(int)\n",
    "\n",
    "# Create the interaction term P_{it} * D_{it}\n",
    "data['PxD'] = data['P'] * data['treatUberX']\n",
    "\n",
    "# Create the interaction term F_{it} * D_{it}\n",
    "data['FxD'] = data['F'] * data['treatUberX']\n",
    "\n",
    "# Create interaction between agency and city\n",
    "data['agency_city'] = data['agency'] + data['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $OLS: log Y_{it} = \\alpha + D_{it}\\beta + W_{it}\\gamma + e_{it}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'dateSurvey' to datetime format\n",
    "data['dateSurvey'] = pd.to_datetime(data['dateSurvey'], errors='coerce')\n",
    "\n",
    "# Set the index to be a MultiIndex for panel data\n",
    "data = data.set_index(['agency_city', 'dateSurvey'])\n",
    "\n",
    "# Define the dependent variable and independent variables\n",
    "Y = np.log(data['UPTTotal'])\n",
    "D = data['treatUberX']\n",
    "W = data[['popestimate', 'employment', 'aveFareTotal', 'VRHTotal', 'VOMSTotal', 'VRMTotal', 'gasPrice']]\n",
    "PxD = data['PxD']\n",
    "FxD = data['FxD']\n",
    "\n",
    "# Scale the independent variables with log transformation\n",
    "W_scaled_df = np.log(W)\n",
    "\n",
    "# Create the design matrices\n",
    "X = pd.concat([D, W_scaled_df], axis=1)\n",
    "\n",
    "# Add constant to the models\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               UPTTotal   R-squared:                       0.833\n",
      "Model:                            OLS   Adj. R-squared:                  0.833\n",
      "Method:                 Least Squares   F-statistic:                 4.463e+04\n",
      "Date:                Fri, 07 Jun 2024   Prob (F-statistic):               0.00\n",
      "Time:                        15:38:47   Log-Likelihood:                -84548.\n",
      "No. Observations:               71768   AIC:                         1.691e+05\n",
      "Df Residuals:                   71759   BIC:                         1.692e+05\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           -0.7976      0.068    -11.676      0.000      -0.931      -0.664\n",
      "treatUberX       0.0382      0.010      3.890      0.000       0.019       0.057\n",
      "popestimate     -0.9271      0.034    -27.371      0.000      -0.994      -0.861\n",
      "employment       0.9909      0.034     29.363      0.000       0.925       1.057\n",
      "aveFareTotal    -0.1277      0.004    -32.388      0.000      -0.135      -0.120\n",
      "VRHTotal         1.3417      0.009    143.916      0.000       1.323       1.360\n",
      "VOMSTotal       -0.2376      0.008    -30.390      0.000      -0.253      -0.222\n",
      "VRMTotal         0.0688      0.011      6.400      0.000       0.048       0.090\n",
      "gasPrice         0.2136      0.013     16.540      0.000       0.188       0.239\n",
      "==============================================================================\n",
      "Omnibus:                     9050.948   Durbin-Watson:                   1.809\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           106297.994\n",
      "Skew:                          -0.089   Prob(JB):                         0.00\n",
      "Kurtosis:                       8.959   Cond. No.                         606.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Fit the OLS model\n",
    "model1 = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Print the results\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $OLS: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta + W_{it}\\gamma + e_{it}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:               UPTTotal   R-squared:                        0.3295\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.9395\n",
      "No. Observations:               71768   R-squared (Within):               0.3476\n",
      "Date:                Fri, Jun 07 2024   R-squared (Overall):              0.9503\n",
      "Time:                        15:38:48   Log-likelihood                   -5543.7\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      4357.3\n",
      "Entities:                         676   P-value                           0.0000\n",
      "Avg Obs:                       106.17   Distribution:                 F(8,70941)\n",
      "Min Obs:                       6.0000                                           \n",
      "Max Obs:                       144.00   F-statistic (robust):             4357.3\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     144   Distribution:                 F(8,70941)\n",
      "Avg Obs:                       498.39                                           \n",
      "Min Obs:                       161.00                                           \n",
      "Max Obs:                       586.00                                           \n",
      "                                                                                \n",
      "                              Parameter Estimates                               \n",
      "================================================================================\n",
      "              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "--------------------------------------------------------------------------------\n",
      "treatUberX      -0.0354     0.0051    -6.9140     0.0000     -0.0454     -0.0254\n",
      "popestimate      0.2789     0.0556     5.0147     0.0000      0.1699      0.3878\n",
      "employment       0.2677     0.0397     6.7474     0.0000      0.1899      0.3455\n",
      "aveFareTotal    -0.0996     0.0030    -33.542     0.0000     -0.1054     -0.0938\n",
      "VRHTotal         0.3052     0.0071     43.084     0.0000      0.2913      0.3190\n",
      "VOMSTotal        0.2314     0.0058     40.090     0.0000      0.2201      0.2427\n",
      "VRMTotal         0.2664     0.0071     37.501     0.0000      0.2525      0.2803\n",
      "gasPrice        -0.0407     0.0394    -1.0337     0.3013     -0.1179      0.0365\n",
      "================================================================================\n",
      "\n",
      "F-test for Poolability: 697.27\n",
      "P-value: 0.0000\n",
      "Distribution: F(818,70941)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "# Ensure Y is a Series rather than a DataFrame\n",
    "Y = Y.squeeze()\n",
    "\n",
    "# Create the design matrices\n",
    "X = pd.concat([D, W_scaled_df], axis=1)\n",
    "\n",
    "# Fit the Panel OLS models with individual and time fixed effects\n",
    "model2 = PanelOLS(Y, X, entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "result2 = model2.fit()\n",
    "\n",
    "# Print the summaries to check the fixed effects inclusion\n",
    "print(result2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $OLS: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}P_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $P_{it}$ is a dummy that takes value 1 if the corresponding MSA has population larger than the median population in the dataset and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:               UPTTotal   R-squared:                        0.0103\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.0109\n",
      "No. Observations:               71768   R-squared (Within):              -0.0043\n",
      "Date:                Fri, Jun 07 2024   R-squared (Overall):             -0.0065\n",
      "Time:                        15:38:50   Log-likelihood                -1.952e+04\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      81.813\n",
      "Entities:                         676   P-value                           0.0000\n",
      "Avg Obs:                       106.17   Distribution:                 F(9,70940)\n",
      "Min Obs:                       6.0000                                           \n",
      "Max Obs:                       144.00   F-statistic (robust):             81.813\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     144   Distribution:                 F(9,70940)\n",
      "Avg Obs:                       498.39                                           \n",
      "Min Obs:                       161.00                                           \n",
      "Max Obs:                       586.00                                           \n",
      "                                                                                \n",
      "                              Parameter Estimates                               \n",
      "================================================================================\n",
      "              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "--------------------------------------------------------------------------------\n",
      "treatUberX      -0.0568     0.0098    -5.8149     0.0000     -0.0759     -0.0376\n",
      "PxD              0.0250     0.0103     2.4153     0.0157      0.0047      0.0452\n",
      "popestimate  -1.135e-07  1.728e-08    -6.5705     0.0000  -1.474e-07  -7.968e-08\n",
      "employment    1.656e-07  2.246e-08     7.3731     0.0000   1.216e-07   2.096e-07\n",
      "aveFareTotal    -0.0012     0.0005    -2.5458     0.0109     -0.0021     -0.0003\n",
      "VRHTotal      8.614e-07  1.693e-07     5.0889     0.0000   5.296e-07   1.193e-06\n",
      "VOMSTotal        0.0005   2.27e-05     20.102     0.0000      0.0004      0.0005\n",
      "VRMTotal     -2.605e-08   7.54e-09    -3.4549     0.0006  -4.083e-08  -1.127e-08\n",
      "gasPrice        -0.0136     0.0167    -0.8127     0.4164     -0.0463      0.0192\n",
      "================================================================================\n",
      "\n",
      "F-test for Poolability: 2031.8\n",
      "P-value: 0.0000\n",
      "Distribution: F(818,70940)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "# Create the design matrices\n",
    "X1 = pd.concat([D, PxD, W], axis=1)\n",
    "\n",
    "# Fit the Panel OLS models with individual and time fixed effects\n",
    "model3 = PanelOLS(Y, X1, entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "result3 = model3.fit()\n",
    "\n",
    "print(result3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $OLS: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}F_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $F_{it}$ is a dummy that takes value 1 if the number of rides of the public travel agency is larger than the median number of rides among all public transit agencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:               UPTTotal   R-squared:                        0.0102\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.0016\n",
      "No. Observations:               71768   R-squared (Within):              -0.0014\n",
      "Date:                Fri, Jun 07 2024   R-squared (Overall):              0.0024\n",
      "Time:                        15:38:52   Log-likelihood                -1.952e+04\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      81.183\n",
      "Entities:                         676   P-value                           0.0000\n",
      "Avg Obs:                       106.17   Distribution:                 F(9,70940)\n",
      "Min Obs:                       6.0000                                           \n",
      "Max Obs:                       144.00   F-statistic (robust):             81.183\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     144   Distribution:                 F(9,70940)\n",
      "Avg Obs:                       498.39                                           \n",
      "Min Obs:                       161.00                                           \n",
      "Max Obs:                       586.00                                           \n",
      "                                                                                \n",
      "                              Parameter Estimates                               \n",
      "================================================================================\n",
      "              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "--------------------------------------------------------------------------------\n",
      "treatUberX      -0.0370     0.0082    -4.5043     0.0000     -0.0531     -0.0209\n",
      "FxD             -0.0040     0.0084    -0.4758     0.6342     -0.0205      0.0125\n",
      "popestimate  -1.041e-07  1.689e-08    -6.1639     0.0000  -1.372e-07  -7.099e-08\n",
      "employment    1.735e-07  2.223e-08     7.8057     0.0000     1.3e-07   2.171e-07\n",
      "aveFareTotal    -0.0012     0.0005    -2.5666     0.0103     -0.0021     -0.0003\n",
      "VRHTotal      8.576e-07  1.693e-07     5.0665     0.0000   5.259e-07   1.189e-06\n",
      "VOMSTotal        0.0005  2.273e-05     20.116     0.0000      0.0004      0.0005\n",
      "VRMTotal     -2.599e-08  7.541e-09    -3.4469     0.0006  -4.077e-08  -1.121e-08\n",
      "gasPrice        -0.0106     0.0167    -0.6348     0.5255     -0.0433      0.0221\n",
      "================================================================================\n",
      "\n",
      "F-test for Poolability: 1907.5\n",
      "P-value: 0.0000\n",
      "Distribution: F(818,70940)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "# Create the design matrices\n",
    "X2 = pd.concat([D, FxD, W], axis=1)\n",
    "\n",
    "# Fit the Panel OLS models with individual and time fixed effects\n",
    "model4 = PanelOLS(Y, X2, entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "result4 = model4.fit()\n",
    "\n",
    "print(result4.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. $LASSO: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}P_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $P_{it}$ is a dummy that takes value 1 if the corresponding MSA has population larger than the median population in the dataset and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"uber_dataset.csv\", index_col=0)\n",
    "\n",
    "# Drop treatGTNotStd variable\n",
    "data = data.drop(columns='treatGTNotStd')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# If treatUberX is greater than 0.5, set it to 1, if not, set it to 0\n",
    "data['treatUberX'] = (data['treatUberX'] > 0.5).astype(int)\n",
    "\n",
    "# Calculate the median population across all entities\n",
    "median_population = data.groupby('dateSurvey')['popestimate'].median()\n",
    "\n",
    "# Merge the median population back to the original dataframe\n",
    "data = data.reset_index().merge(median_population.rename('median_pop'), on='dateSurvey')\n",
    "\n",
    "# Create the dummy variable P_{it}\n",
    "data['P'] = (data['popestimate'] > data['median_pop']).astype(int)\n",
    "\n",
    "# Calculate the median rides across all times\n",
    "median_rides = data.groupby('dateSurvey')['UPTTotal'].median()\n",
    "\n",
    "# Merge the median population back to the original dataframe\n",
    "data = data.reset_index().merge(median_rides.rename('median_ride'), on='dateSurvey')\n",
    "\n",
    "# Create the dummy variable F_{it}\n",
    "data['F'] = (data['UPTTotal'] > data['median_ride']).astype(int)\n",
    "\n",
    "# Create the interaction term P_{it} * D_{it}\n",
    "data['PxD'] = data['P'] * data['treatUberX']\n",
    "\n",
    "# Create the interaction term F_{it} * D_{it}\n",
    "data['FxD'] = data['F'] * data['treatUberX']\n",
    "\n",
    "# Create interaction between agency and city\n",
    "data['agency_city'] = data['agency'] + data['city']\n",
    "\n",
    "# Define the dependent variable and independent variables\n",
    "Y = np.log(data['UPTTotal'])\n",
    "D = data['treatUberX']\n",
    "W = data[['popestimate', 'employment', 'aveFareTotal', 'VRHTotal', 'VOMSTotal', 'VRMTotal', 'gasPrice']]\n",
    "PxD = data['PxD']\n",
    "FxD = data['FxD']\n",
    "\n",
    "# Scale the independent variables with log transformation\n",
    "W_scaled_df = np.log(W)\n",
    "\n",
    "# Create the design matrices\n",
    "X = pd.concat([D, W_scaled_df], axis=1)\n",
    "\n",
    "# Encode entity and time as dummy variables\n",
    "entity_dummies = pd.get_dummies(data['agency_city'], drop_first=True)\n",
    "time_dummies = pd.get_dummies(data['dateSurvey'], drop_first=True)\n",
    "\n",
    "# Create the design matrices\n",
    "X3 = np.column_stack((D, PxD, W_scaled_df, entity_dummies, time_dummies))\n",
    "Y = np.log(data['UPTTotal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Coefficients:\n",
      "        Feature  Coefficient\n",
      "0             D     0.000000\n",
      "1             P     0.000000\n",
      "2   popestimate     0.000000\n",
      "3    employment     0.009711\n",
      "4  aveFareTotal    -0.000000\n",
      "5      VRHTotal     1.170157\n",
      "6     VOMSTotal     0.000000\n",
      "7      VRMTotal     0.000000\n",
      "8      gasPrice     0.000000\n"
     ]
    }
   ],
   "source": [
    "# Fit Lasso regression models\n",
    "alpha1 = BCCH(X3, Y)\n",
    "lasso1 = Lasso(alpha=alpha1)  # You can adjust the alpha parameter as needed\n",
    "lasso1.fit(X3, Y)\n",
    "\n",
    "# Define the feature names\n",
    "feature_names = ['D', 'P', 'popestimate', 'employment', 'aveFareTotal', 'VRHTotal', 'VOMSTotal', 'VRMTotal', 'gasPrice']\n",
    "\n",
    "# Create DataFrame for Model 1\n",
    "coef1_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso1.coef_[:9]\n",
    "})\n",
    "\n",
    "print(\"Model 1 Coefficients:\")\n",
    "print(coef1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. $LASSO: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}F_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $F_{it}$ is a dummy that takes value 1 if the number of rides of the public travel agency is larger than the median number of rides among all public transit agencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Coefficients:\n",
      "        Feature  Coefficient\n",
      "0             D     0.000000\n",
      "1             F     0.000000\n",
      "2   popestimate     0.000000\n",
      "3    employment     0.009711\n",
      "4  aveFareTotal    -0.000000\n",
      "5      VRHTotal     1.170157\n",
      "6     VOMSTotal     0.000000\n",
      "7      VRMTotal     0.000000\n",
      "8      gasPrice     0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create the design matrices\n",
    "X4 = np.column_stack((D, FxD, W_scaled_df, entity_dummies, time_dummies))\n",
    "\n",
    "# Fit Lasso regression models\n",
    "alpha2 = BCCH(X4, Y)\n",
    "lasso2 = Lasso(alpha=alpha1)  # You can adjust the alpha parameter as needed\n",
    "lasso2.fit(X4, Y)\n",
    "\n",
    "# Define the feature names\n",
    "feature_names = ['D', 'F', 'popestimate', 'employment', 'aveFareTotal', 'VRHTotal', 'VOMSTotal', 'VRMTotal', 'gasPrice']\n",
    "\n",
    "# Create DataFrame for Model 1\n",
    "coef2_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso2.coef_[:9]\n",
    "})\n",
    "\n",
    "print(\"Model 2 Coefficients:\")\n",
    "print(coef2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. $Double-LASSO: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}P_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $P_{it}$ is a dummy that takes value 1 if the corresponding MSA has population larger than the median population in the dataset and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.0231\n",
      "Estimated standard error: 0.0159\n",
      "Confidence interval: (-0.0081, 0.0542)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(np.log(data['UPTTotal']), ndmin=1).T\n",
    "D = np.array(data['treatUberX'], ndmin=1).T\n",
    "W = np.array(np.log(data[['popestimate', 'employment', 'aveFareTotal', 'VRHTotal', 'VOMSTotal', 'VRMTotal', 'gasPrice']]))\n",
    "P = np.array(data['P'], ndmin=1).T\n",
    "W1 = np.column_stack((D*P, W))\n",
    "W2 = np.column_stack((D, W))\n",
    "DP = D * P\n",
    "\n",
    "# Convert dummy variables to numpy arrays\n",
    "entity_dummies_array = entity_dummies.to_numpy()\n",
    "time_dummies_array = time_dummies.to_numpy()\n",
    "\n",
    "# Concatenate the arrays\n",
    "W1_combined = np.concatenate([W1, entity_dummies_array, time_dummies_array], axis=1)\n",
    "W2_combined = np.concatenate([W2, entity_dummies_array, time_dummies_array], axis=1)\n",
    "\n",
    "\n",
    "# Run double LASSO regression to estimate alpha for D\n",
    "estimated_alpha, estimated_std_error = double_lasso(Y, D, W1_combined)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error:\", estimated_std_error.round(4))\n",
    "min = estimated_alpha - 1.96 * estimated_std_error\n",
    "max = estimated_alpha + 1.96 * estimated_std_error\n",
    "print(\"Confidence interval:\", (min.round(4), max.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.0157\n",
      "Estimated standard error: 0.0217\n",
      "Confidence interval: (-0.0268, 0.0583)\n"
     ]
    }
   ],
   "source": [
    "# Run double LASSO regression to estimate alpha for D*P\n",
    "estimated_alpha, estimated_std_error = double_lasso(Y, DP, W2_combined)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error:\", estimated_std_error.round(4))\n",
    "min = estimated_alpha - 1.96 * estimated_std_error\n",
    "max = estimated_alpha + 1.96 * estimated_std_error\n",
    "print(\"Confidence interval:\", (min.round(4), max.round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. $Double-LASSO: log Y_{it} = \\eta_i + \\delta_t + D_{it}\\beta_{1} + D_{it}F_{it}\\beta_{2} + W_{it}\\gamma + e_{it}$; where $F_{it}$ is a dummy that takes value 1 if the number of rides of the public travel agency is larger than the median number of rides among all public transit agencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.1614\n",
      "Estimated standard error: 0.0163\n",
      "Confidence interval: (-0.1934, -0.1294)\n"
     ]
    }
   ],
   "source": [
    "F = np.array(data['F'], ndmin=1).T\n",
    "W3 = np.column_stack((D*F, W))\n",
    "W3_combined = np.concatenate([W3, entity_dummies_array, time_dummies_array], axis=1)\n",
    "DF = D * F\n",
    "\n",
    "# Run double LASSO regression to estimate alpha for D, using F as an instrument\n",
    "estimated_alpha, estimated_std_error = double_lasso(Y, D, W3_combined)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error:\", estimated_std_error.round(4))\n",
    "min = estimated_alpha - 1.96 * estimated_std_error\n",
    "max = estimated_alpha + 1.96 * estimated_std_error\n",
    "print(\"Confidence interval:\", (min.round(4), max.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.5132\n",
      "Estimated standard error: 0.0209\n",
      "Confidence interval: (0.4723, 0.5542)\n"
     ]
    }
   ],
   "source": [
    "# Run double LASSO regression to estimate alpha for D*P\n",
    "estimated_alpha, estimated_std_error = double_lasso(Y, DF, W2_combined)\n",
    "print(\"Estimated alpha:\", estimated_alpha.round(4))\n",
    "print(\"Estimated standard error:\", estimated_std_error.round(4))\n",
    "min = estimated_alpha - 1.96 * estimated_std_error\n",
    "max = estimated_alpha + 1.96 * estimated_std_error\n",
    "print(\"Confidence interval:\", (min.round(4), max.round(4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
